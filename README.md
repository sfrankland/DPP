# DPP CogSci 2020 - ReadMe

Matlab code for Frankland & Cohen (2020) - Determinantal Point Processes for Memory and Structured Inference

Determinantal Point Processes (DPPs) are probabilistic
models of repulsion, capturing negative dependencies
between states. Here, we show that a DPP in
representation-space predicts inferential biases toward
mutual exclusivity commonly observed in word learning
(mutual exclusivity bias) and reasoning (disjunctive
syllogism) tasks. The DPP attempts to
maximize the total ”volume” spanned by the set of
inferred code-vectors. In a representational system in
which combinatorial codes are constructed by re-using
components, a DPP will naturally favor the combination
of previously un-used components.

We suggest that this bias toward the selection of volume-maximizing
combinations may exist to promote the efficient retrieval
of individuals from memory. In support of this, we show
the same algorithm implements efficient ”hashing”,
minimizing collisions between key/value pairs without
expanding the required storage space. We suggest
that the mechanisms that promote efficient memory
search may also underlie cognitive biases in structured
inference
